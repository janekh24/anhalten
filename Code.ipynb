{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "edoLjt7DhcbV"
      },
      "outputs": [],
      "source": [
        "!pip install datasets evaluate transformers[sentencepiece]\n",
        "!pip install -U adapter-transformers\n",
        "!pip install -U datasets\n",
        "!pip install pytorch-pretrained-bert\n",
        "!pip install nlp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K3y1ZIB8hjR7"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vis4AyBnhukw"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "import codecs\n",
        "import json\n",
        "import spacy\n",
        "from sklearn.metrics import classification_report, accuracy_score, hamming_loss, \\\n",
        "    f1_score, precision_score, recall_score, average_precision_score, roc_auc_score, confusion_matrix, \\\n",
        "    brier_score_loss\n",
        "import numpy as np\n",
        "\n",
        "# Ihre Auswertungsfunktion\n",
        "def evaluate_and_save_to_csv():\n",
        "    # Annahme: Sie haben bereits predy, testy und andere erforderliche Variablen\n",
        "    acc, f1, precision, recall, gmean, bss, roc_auc, info = binary_eval(predy, testy, verbose=True, return_f1=True, predscore=None)\n",
        "\n",
        "    # Erstellen Sie ein DataFrame aus den Ergebnissen\n",
        "    results_df = pd.DataFrame({\n",
        "        'Accuracy': [acc],\n",
        "        'F1_Score_0': [f1[0]],\n",
        "        'F1_Score_1': [f1[1]],\n",
        "        'Precision_0': [precision[0]],\n",
        "        'Precision_1': [precision[1]],\n",
        "        'Recall_0': [recall[0]],\n",
        "        'Recall_1': [recall[1]],\n",
        "        'G-Mean': [gmean],\n",
        "        'Brier_Score': [bss],\n",
        "        'ROC_AUC': [roc_auc]\n",
        "    })\n",
        "\n",
        "    # Speichern Sie das DataFrame in eine CSV-Datei\n",
        "    results_df.to_csv('/content/drive/MyDrive/Modelle/results.csv', index=False)\n",
        "\n",
        "def binary_eval(predy, testy, verbose=True, return_f1=False, predscore=None):\n",
        "    acc = accuracy_score(testy, predy)\n",
        "    f1 = f1_score(testy, predy, average=None)\n",
        "    precision = precision_score(testy, predy, average=None)\n",
        "    recall = recall_score(testy, predy, average=None)\n",
        "    epsilon = 1e-8\n",
        "\n",
        "    htn, hfp, hfn, htp = confusion_matrix(testy, predy).ravel()\n",
        "    hsensi = htp / (htp + hfn + epsilon)\n",
        "    hspec = htn / (hfp + htn + epsilon)\n",
        "    gmean = np.sqrt(hsensi*hspec)\n",
        "\n",
        "\n",
        "    info = \"Acc : {}\\nf1 : {}\\nprecision : {}\\nrecall : {}\\nG-mean : {}\".format(acc,\n",
        "            \" \".join([str(x) for x in f1]), \" \".join([str(x) for x in precision]),\n",
        "            \" \".join([str(x) for x in recall]), gmean)\n",
        "\n",
        "    if predscore is not None:\n",
        "        bss = brier_score_loss(testy, predscore)\n",
        "        roc_auc = roc_auc_score(testy, predscore)\n",
        "        info += \"\\nbss : {}\\nROC-AUC : {}\".format(bss, roc_auc)\n",
        "\n",
        "    if verbose:\n",
        "        print(info)\n",
        "\n",
        "    if return_f1:\n",
        "        return acc, f1, precision, recall, gmean, bss, roc_auc, info\n",
        "    else:\n",
        "        return acc, info\n",
        "\n",
        "\n",
        "def subsets(nums):\n",
        "    \"\"\"\n",
        "    :type nums: List[int]\n",
        "    :rtype: List[List[int]]\n",
        "    \"\"\"\n",
        "    ans = []\n",
        "    def dfs(curpos, tmp):\n",
        "        if tmp:\n",
        "            ans.append(tmp[:])\n",
        "        for i in range(curpos, len(nums)):\n",
        "            tmp.append(nums[i])\n",
        "            dfs(i+1, tmp)\n",
        "            tmp.pop(-1)\n",
        "    dfs(0, [])\n",
        "    return ans\n",
        "\n",
        "\n",
        "def sent_ner_bounds(sen, nlp=None):\n",
        "    if nlp is None:\n",
        "        nlp = spacy.load('en')\n",
        "    tokens, tags = [], []\n",
        "    print(sen)\n",
        "    for doc in nlp.pipe([sen]):\n",
        "        for token in doc:\n",
        "            tags.append(token.ent_iob_)\n",
        "            tokens.append(str(token))\n",
        "\n",
        "    rep_pos = []\n",
        "    vis = [False for _ in range(len(tags))]\n",
        "    for idx, tag in enumerate(tags):\n",
        "        if tag == 'O':\n",
        "            rep_pos.append([idx, idx])\n",
        "            vis[idx] = True\n",
        "        elif tag == 'B':\n",
        "            end = idx\n",
        "            for j in range(idx+1, len(tags)):\n",
        "                if tags[j] == 'I':\n",
        "                    end = j\n",
        "                else:\n",
        "                    break\n",
        "            rep_pos.append([idx, end])\n",
        "        elif tag == 'I':\n",
        "            continue\n",
        "\n",
        "    return ' '.join(tokens), rep_pos\n",
        "\n",
        "\n",
        "def remove_marked_sen(sen, start_id, end_id):\n",
        "    tokens = sen if type(sen) == list else sen.strip().split()\n",
        "    if tokens[start_id].startswith(\"===\") and tokens[end_id].endswith(\"===\"):\n",
        "        tokens[start_id] = tokens[start_id][3:]\n",
        "        tokens[end_id] = tokens[end_id][:-3]\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZYt-kHdh1Wy"
      },
      "outputs": [],
      "source": [
        "from torch.utils import data\n",
        "import torch\n",
        "import numpy as np\n",
        "from tqdm import tqdm, trange\n",
        "import collections\n",
        "import codecs\n",
        "import json\n",
        "\n",
        "\n",
        "class InputExample(object):\n",
        "\n",
        "    def __init__(self, guid, sen, idxs, label):\n",
        "        self.guid = guid\n",
        "        self.sen = sen\n",
        "        self.idxs = idxs\n",
        "        self.label = label\n",
        "\n",
        "\n",
        "class InputFeatures(object):\n",
        "\n",
        "    def __init__(self, guid, input_ids, input_mask, segment_ids,  predict_mask, label_id):\n",
        "        self.guid = guid\n",
        "        self.input_ids = input_ids\n",
        "        self.input_mask = input_mask\n",
        "        self.segment_ids = segment_ids\n",
        "        self.predict_mask = predict_mask\n",
        "        self.label_id = label_id\n",
        "\n",
        "\n",
        "class DataProcessor(object):\n",
        "    def __init__(self):\n",
        "        self.num_consist = 0\n",
        "        self.num_hallu = 0\n",
        "\n",
        "    def _read_data(self, input_file, require_uidx=False):\n",
        "        with open(input_file) as f:\n",
        "            # out_lines = []\n",
        "            out_lists = []\n",
        "            entries = f.read().strip().split(\"\\n\")\n",
        "            for entry in entries:\n",
        "                example = json.loads(entry.strip())\n",
        "                if \"hallucination\" not in example:\n",
        "                    label = -1\n",
        "                else:\n",
        "                    label = example[\"hallucination\"]\n",
        "                    if label not in [0, 1]:\n",
        "                        continue\n",
        "                if require_uidx:\n",
        "                    sen, token_ids, uidx = example[\"replaced\"], example[\"replaced_ids\"], example[\"idx\"]\n",
        "                    out_lists.append([sen, token_ids, label, uidx])\n",
        "                else:\n",
        "                    sen, token_ids = example[\"replaced\"], example[\"replaced_ids\"]\n",
        "                    out_lists.append([sen, token_ids, label])\n",
        "        return out_lists\n",
        "\n",
        "    def _create_examples(self, all_lists):\n",
        "        examples = []\n",
        "        for (i, one_lists) in enumerate(all_lists):\n",
        "            guid = i\n",
        "            if len(one_lists) == 3:  # Don't contain key \"idx\" in json file\n",
        "                sen, token_ids, label = one_lists\n",
        "            elif len(one_lists) == 4:  # Contain key \"idx\" in json file\n",
        "                sen, token_ids, label, guid = one_lists\n",
        "            else:\n",
        "                assert len(one_lists) == 3 or len(one_lists) == 4\n",
        "\n",
        "            if label == 0:\n",
        "                self.num_consist += 1\n",
        "            elif label == 1:\n",
        "                self.num_hallu += 1\n",
        "\n",
        "            examples.append(InputExample(\n",
        "                guid=guid, sen=sen, idxs=token_ids, label=label))\n",
        "        return examples\n",
        "\n",
        "    def get_examples(self, path, require_uidx=False):\n",
        "        return self._create_examples(\n",
        "            self._read_data(path, require_uidx))\n",
        "\n",
        "    def get_label_dist(self):\n",
        "        return [self.num_consist, self.num_hallu]\n",
        "\n",
        "def truncate(rep_subtokens, predict_mask, max_seq_length, rep_start_id, rep_end_id, mode=\"online\"):\n",
        "    '''\n",
        "    Truncate the sequence if given a fixed context window. For example, given the following input sentence:\n",
        "    \"he signed a professional contract and promoted to the ===senior team=== where he managed to play for almost 3 years .\"\n",
        "    if the context window length is set as 4, the function will truncate the input as follows:\n",
        "\n",
        "    online mode: \"and promoted to the ===senior team===\"\n",
        "    offline mode: \"to the ===senior team=== where he\"\n",
        "    '''\n",
        "\n",
        "    position = 0\n",
        "    one = 0\n",
        "\n",
        "    for x in predict_mask:\n",
        "      if one == 0:\n",
        "        if predict_mask[position] == 1:\n",
        "          rep_start_id = position\n",
        "          one = 1\n",
        "          position = position + 1\n",
        "        else:\n",
        "          position = position + 1\n",
        "      else:\n",
        "        if predict_mask[position] == 1:\n",
        "          position = position + 1\n",
        "        else:\n",
        "          rep_end_id = position - 1\n",
        "          break\n",
        "\n",
        "    if mode == \"offline\":\n",
        "        if len(rep_subtokens) > max_seq_length - 2:\n",
        "            mid_pt = int((rep_start_id + rep_end_id) / 2)\n",
        "            left_seq_length = int(max_seq_length / 2)\n",
        "            right_seq_length = max_seq_length - left_seq_length\n",
        "            if mid_pt - left_seq_length >= 0 and mid_pt + right_seq_length < len(rep_subtokens):\n",
        "                left_pt = mid_pt - left_seq_length\n",
        "                right_pt = mid_pt + right_seq_length\n",
        "            elif mid_pt - left_seq_length < 0 and mid_pt + right_seq_length < len(rep_subtokens):\n",
        "                left_pt = 0\n",
        "                right_pt = max_seq_length\n",
        "            elif mid_pt - left_seq_length >= 0 and mid_pt + right_seq_length >= len(rep_subtokens):\n",
        "                right_pt = len(rep_subtokens)\n",
        "                left_pt = len(rep_subtokens) - max_seq_length\n",
        "            elif mid_pt - left_seq_length < 0 and mid_pt + right_seq_length >= len(rep_subtokens):\n",
        "                left_pt = 0\n",
        "                right_pt = len(rep_subtokens)\n",
        "            rep_subtokens = rep_subtokens[left_pt:right_pt - 1]\n",
        "            predict_mask = predict_mask[left_pt:right_pt - 1]\n",
        "    else: # online\n",
        "        left_pt, right_pt = 0, rep_end_id + 1\n",
        "        if right_pt > max_seq_length - 2:\n",
        "            left_pt = right_pt - (max_seq_length - 2)\n",
        "        rep_subtokens = rep_subtokens[left_pt:right_pt]\n",
        "        predict_mask = predict_mask[left_pt:right_pt]\n",
        "    return rep_subtokens, predict_mask\n",
        "\n",
        "\n",
        "def example2feature(example, tokenizer, max_seq_length, model_name, mode=\"online\"):\n",
        "    rep_start_id, rep_end_id = example.idxs\n",
        "    rep_tokens = remove_marked_sen(example.sen, rep_start_id, rep_end_id)\n",
        "\n",
        "    if 'xlnet' in model_name.lower():\n",
        "        rep_subtokens = []\n",
        "        predict_mask = []\n",
        "\n",
        "        for id, rep_token in enumerate(rep_tokens):\n",
        "            rep_subtoken = tokenizer.tokenize(rep_token)\n",
        "            if id >= rep_start_id and id <= rep_end_id:\n",
        "                rep_subtokens.extend(rep_subtoken)\n",
        "                predict_mask.extend(len(rep_subtoken) * [1])\n",
        "            else:\n",
        "                rep_subtokens.extend(rep_subtoken)\n",
        "                predict_mask.extend(len(rep_subtoken) * [0])\n",
        "\n",
        "        rep_subtokens, predict_mask = truncate(rep_subtokens, predict_mask, max_seq_length, rep_start_id, rep_end_id, mode=mode)\n",
        "\n",
        "        rep_subtokens.extend([\"<sep>\", \"<cls>\"])\n",
        "        predict_mask.extend([0, 0])\n",
        "\n",
        "    elif 'gpt' not in model_name.lower():\n",
        "\n",
        "        rep_subtokens = []\n",
        "        predict_mask = []\n",
        "        for id, rep_token in enumerate(rep_tokens):\n",
        "            rep_subtoken = tokenizer.tokenize(rep_token)\n",
        "            if id >= rep_start_id and id <= rep_end_id:\n",
        "                rep_subtokens.extend(rep_subtoken)\n",
        "                predict_mask.extend(len(rep_subtoken) * [1])\n",
        "            else:\n",
        "                rep_subtokens.extend(rep_subtoken)\n",
        "                predict_mask.extend(len(rep_subtoken) * [0])\n",
        "\n",
        "        rep_subtokens, predict_mask = truncate(rep_subtokens, predict_mask, max_seq_length, rep_start_id, rep_end_id, mode=mode)\n",
        "\n",
        "        rep_subtokens.insert(0, \"[CLS]\")\n",
        "        predict_mask.insert(0, 0)\n",
        "        rep_subtokens.append('[SEP]')\n",
        "        predict_mask.append(0)\n",
        "\n",
        "    elif 'gpt' in model_name.lower():\n",
        "        rep_subtokens = []\n",
        "        predict_mask = []\n",
        "\n",
        "        for id, rep_token in enumerate(rep_tokens):\n",
        "            rep_token = \" \"+rep_token if id!=0 else rep_token\n",
        "            rep_subtoken = tokenizer.tokenize(rep_token)\n",
        "            if id >= rep_start_id and id <= rep_end_id:\n",
        "                rep_subtokens.extend(rep_subtoken)\n",
        "                predict_mask.extend(len(rep_subtoken) * [1])\n",
        "            else:\n",
        "                rep_subtokens.extend(rep_subtoken)\n",
        "                predict_mask.extend(len(rep_subtoken) * [0])\n",
        "\n",
        "        rep_subtokens, predict_mask = truncate(rep_subtokens, predict_mask, max_seq_length, rep_start_id, rep_end_id, mode=mode)\n",
        "\n",
        "    input_ids = tokenizer.convert_tokens_to_ids(rep_subtokens)\n",
        "    segment_ids = [0] * len(input_ids)\n",
        "    input_mask = [1] * len(input_ids)\n",
        "\n",
        "    feat=InputFeatures(\n",
        "                guid=example.guid,\n",
        "                # tokens=tokens,\n",
        "                input_ids=input_ids,\n",
        "                input_mask=input_mask,\n",
        "                segment_ids=segment_ids,\n",
        "                predict_mask=predict_mask,\n",
        "                label_id=example.label)\n",
        "    return feat\n",
        "\n",
        "def get_examples_from_sen_tuple(sen, rep_pos):\n",
        "    examples = []\n",
        "    for uid, pos in enumerate(rep_pos):\n",
        "        examples.append(InputExample(guid=uid, sen=sen, idxs=pos, label=0))\n",
        "    return examples\n",
        "\n",
        "class HalluDataset(data.Dataset):\n",
        "    def __init__(self, examples, tokenizer, max_seq_length, model_name, task_mode=\"online\"):\n",
        "        self.examples=examples\n",
        "        self.tokenizer=tokenizer\n",
        "        self.max_seq_length=max_seq_length\n",
        "        self.model_name = model_name\n",
        "        self.task_mode = task_mode\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        feat=example2feature(self.examples[idx], self.tokenizer, self.max_seq_length,\n",
        "                             self.model_name, self.task_mode)\n",
        "        return feat.input_ids, feat.input_mask, feat.segment_ids, feat.predict_mask, feat.label_id, feat.guid\n",
        "\n",
        "    @classmethod\n",
        "    def pad(cls, batch):\n",
        "\n",
        "        seqlen_list = [len(sample[0]) for sample in batch]\n",
        "        maxlen = np.array(seqlen_list).max()\n",
        "\n",
        "        f = lambda x, seqlen: [sample[x] + [0] * (seqlen - len(sample[x])) for sample in batch] # 0: X for padding\n",
        "        input_ids_list = torch.LongTensor(f(0, maxlen))\n",
        "        input_mask_list = torch.LongTensor(f(1, maxlen))\n",
        "        segment_ids_list = torch.LongTensor(f(2, maxlen))\n",
        "        predict_mask_list = torch.ByteTensor(f(3, maxlen))\n",
        "        label_id = torch.LongTensor([sample[4] for sample in batch])\n",
        "        guids = [sample[5] for sample in batch]\n",
        "\n",
        "        return input_ids_list, input_mask_list, segment_ids_list, predict_mask_list, label_id, guids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "90NhMGKWEfd_"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer, BertForMaskedLM, AutoModelWithLMHead, AutoAdapterModel, AdapterConfig, AdapterType, AutoTokenizer, BertConfig, AutoConfig, \\\n",
        "    XLNetLMHeadModel, DebertaTokenizer, DebertaModel\n",
        "from pytorch_pretrained_bert.optimization import BertAdam\n",
        "from transformers import AdapterConfig\n",
        "from transformers.adapters.composition import Stack\n",
        "import torch\n",
        "from nltk import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import random, os, sys\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import codecs\n",
        "import argparse\n",
        "import spacy\n",
        "import numpy as np\n",
        "import matplotlib.cm as cm\n",
        "import matplotlib.colors as color\n",
        "import matplotlib.pyplot as plt\n",
        "from nlp import load_dataset\n",
        "from collections import defaultdict\n",
        "import json\n",
        "import csv\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import random\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn import svm\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "from torch.utils import data\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "class ClfModel(nn.Module):\n",
        "      def __init__(self, args):\n",
        "            super().__init__()\n",
        "\n",
        "            self.load_model = args.load_model\n",
        "\n",
        "            if \"xlnet\" in args.load_model:\n",
        "                self.tokenizer = AutoTokenizer.from_pretrained(self.load_model)\n",
        "                self.model = XLNetLMHeadModel.from_pretrained(self.load_model, mem_len=1024).to(args.device)\n",
        "            else:\n",
        "                self.tokenizer = AutoTokenizer.from_pretrained(self.load_model)\n",
        "                config = AutoConfig.from_pretrained(self.load_model)\n",
        "                config.output_hidden_states = True\n",
        "                self.model = AutoAdapterModel.from_pretrained(self.load_model, config=config)\n",
        "\n",
        "            lang_adapter_config = AdapterConfig.load(\"pfeiffer\", non_linearity=\"gelu\", reduction_factor=2)\n",
        "            adapter_en = self.model.load_adapter(\"en/wiki@ukp\", config=lang_adapter_config)\n",
        "\n",
        "            self.model.add_adapter(\"halluClass\")\n",
        "            self.model.add_classification_head(\"halluClass\")\n",
        "            self.model.train_adapter([\"halluClass\"])\n",
        "            self.model.active_adapters = Stack(\"en\", \"halluClass\")\n",
        "            self.model = self.model.to(args.device)\n",
        "\n",
        "            hidden_size = 1024 if \"large\" in self.load_model or self.load_model==\"gpt2-medium\" else 768\n",
        "\n",
        "            self.hidden2label = nn.Sequential(\n",
        "                                    nn.Linear(hidden_size, hidden_size//2),\n",
        "                                    nn.Sigmoid(),\n",
        "                                    nn.Linear(hidden_size//2, 2)).to(args.device)\n",
        "\n",
        "            # self.hidden2label = nn.Linear(hidden_size, 2).to(args.device)\n",
        "            self.dropout = torch.nn.Dropout(args.dropout)\n",
        "            self.layer = args.bert_layer\n",
        "\n",
        "            self.eval()\n",
        "            self.device = args.device\n",
        "            self.args = args\n",
        "\n",
        "      def replace_adapter(self):\n",
        "        lang_adapter_config = AdapterConfig.load(\"pfeiffer\", non_linearity=\"gelu\", reduction_factor=2)\n",
        "        adapter_de = self.model.load_adapter(\"de/wiki@ukp\", config=lang_adapter_config)\n",
        "        print(\"deutscher Sprachadapter\")\n",
        "        self.model.active_adapters = Stack(\"de\", \"halluClass\")\n",
        "\n",
        "      def english_again(self):\n",
        "        lang_adapter_config = AdapterConfig.load(\"pfeiffer\", non_linearity=\"gelu\", reduction_factor=2)\n",
        "        adapter_en = self.model.load_adapter(\"en/wiki@ukp\", config=lang_adapter_config)\n",
        "        print(\"englischer Sprachadapter\")\n",
        "        self.model.active_adapters = Stack(\"en\", \"halluClass\")\n",
        "\n",
        "      def save_model(self, path):\n",
        "        checkpoint = {\n",
        "            \"model_state_dict\": self.state_dict(),\n",
        "            \"optim_state_dict\": optimizer.state_dict(),\n",
        "            \"args\": self.args,\n",
        "        }\n",
        "        torch.save(checkpoint, path)\n",
        "\n",
        "      def model_run(self, optim):\n",
        "            trainpath = os.path.join(self.args.data_path, \"noisyT4.txt\")\n",
        "\n",
        "            prefix = \"runs/{}_lr_{}_dp_{}_{}_clen{}/\".format(self.load_model, self.args.lr,\n",
        "                                            self.args.dropout, self.args.task_mode, self.args.context_len)\n",
        "            bestmodelpath = prefix + \"best_model.pt\"\n",
        "            epoch, epoch_start = self.args.train_epoch, 1\n",
        "            if os.path.exists(bestmodelpath) and self.args.continue_train:\n",
        "                checkpoint = torch.load(bestmodelpath)\n",
        "                self.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "                epoch_start = checkpoint[\"epoch\"] + 1\n",
        "\n",
        "            writer = SummaryWriter(prefix)\n",
        "            csvlogger = prefix + \"valid_log.csv\"\n",
        "\n",
        "            if not os.path.exists(csvlogger):\n",
        "                csvfile = open(csvlogger, 'w+')\n",
        "                fileHeader = [\"epoch\", \"H_p\", \"H_r\", \"H_f1\", \"C_p\", \"C_r\", \"C_f1\", \"Gmean\",\n",
        "                              \"Acc\", \"BSS\", \"ROC_AUC\"]\n",
        "                csvwriter = csv.writer(csvfile)\n",
        "                csvwriter.writerow(fileHeader)\n",
        "            else:\n",
        "                csvfile = open(csvlogger, 'a')\n",
        "                csvwriter = csv.writer(csvfile)\n",
        "\n",
        "            dp = DataProcessor()\n",
        "            train_examples = dp.get_examples(trainpath)\n",
        "\n",
        "            train_dataset = HalluDataset(train_examples, self.tokenizer, self.args.context_len,\n",
        "                                         self.load_model, self.args.task_mode)\n",
        "\n",
        "            train_dataloader = data.DataLoader(dataset=train_dataset,\n",
        "                                               batch_size=self.args.batch_size,\n",
        "                                               shuffle=True,\n",
        "                                               num_workers=4,\n",
        "                                               collate_fn=HalluDataset.pad)\n",
        "            nSamples = dp.get_label_dist()\n",
        "            print(\"====Train label : {}\".format(nSamples))\n",
        "            normedWeights = [1 - (x / sum(nSamples)) for x in nSamples]\n",
        "            normedWeights = torch.FloatTensor(normedWeights).to(self.args.device)\n",
        "            loss_func = nn.CrossEntropyLoss(weight=normedWeights).to(self.args.device)\n",
        "            fwd_func = self.model_train\n",
        "            best_acc, best_f1_score = -1, -1\n",
        "            for ei in range(epoch_start, epoch+1):\n",
        "                cnt = 0\n",
        "                self.train()\n",
        "                train_loss = 0\n",
        "                predy, trainy, hallu_sm_score = [], [], []\n",
        "                for step, batch in enumerate(train_dataloader):\n",
        "                    batch = tuple(t.to(self.device) for t in batch[:-1])\n",
        "                    input_ids, input_mask, segment_ids, predict_mask, label_ids = batch\n",
        "                    score = fwd_func(input_ids, input_mask, segment_ids, predict_mask)\n",
        "                    hallu_sm = F.softmax(score, dim=1)[:, 1]\n",
        "                    _, pred = torch.max(score, dim=1)\n",
        "                    # print(\"pred {}\".format(pred.size()))\n",
        "                    # print(label_ids.tolist())\n",
        "                    # print(pred.tolist())\n",
        "                    trainy.extend(label_ids.tolist())\n",
        "                    predy.extend(pred.tolist())\n",
        "                    hallu_sm_score.extend(hallu_sm.tolist())\n",
        "                    loss = loss_func(score, label_ids)\n",
        "                    train_loss += loss.item()\n",
        "                    optim.zero_grad()\n",
        "                    loss.backward()\n",
        "                    optim.step()\n",
        "                    cnt += 1\n",
        "                    if cnt % 10 == 0:\n",
        "                        print(\"Training Epoch {} - {:.2f}% - Loss : {}\".format(ei, 100.0 * cnt/len(train_dataloader), train_loss/cnt))\n",
        "                print(\"Training Epoch {} ...\".format(ei))\n",
        "                acc, f1, precision, recall, _, _, _, _ = \\\n",
        "                    binary_eval(predy, trainy, return_f1=True, predscore=hallu_sm_score)\n",
        "                writer.add_scalar('Loss/train_epoch', train_loss, ei)\n",
        "                writer.add_scalar('F1/train_consistent_epoch', f1[0], ei)\n",
        "                writer.add_scalar('Precision/train_consistent_epoch', precision[0], ei)\n",
        "                writer.add_scalar('Recall/train_consistent_epoch', recall[0], ei)\n",
        "                writer.add_scalar('F1/train_hallucination_epoch', f1[1], ei)\n",
        "                writer.add_scalar('Precision/train_hallucination_epoch', precision[1], ei)\n",
        "                writer.add_scalar('Recall/train_hallucination_epoch', recall[1], ei)\n",
        "                writer.add_scalar('Acc/train_epoch', acc, ei)\n",
        "                print(\"Train Epoch {} end ! Loss : {}\".format(ei, train_loss))\n",
        "\n",
        "                if ei % 4 == 0:\n",
        "                    savemodel_path = prefix + \"model_{}_{}_{}.pt\".format(ei, f1[0], f1[1])\n",
        "                    torch.save(\n",
        "                    {\"model_state_dict\": self.state_dict(),\n",
        "                     \"optim_state_dict\": optim.state_dict(),\n",
        "                     \"train_f1\": f1,\n",
        "                     \"train_precision\": precision,\n",
        "                     \"train_recall\": recall,\n",
        "                     \"train_acc\": acc,\n",
        "                     \"epoch\": epoch},\n",
        "                     savemodel_path)\n",
        "\n",
        "                validpath = os.path.join(self.args.data_path, \"noisyV4.txt\")\n",
        "                valid_examples = dp.get_examples(validpath)\n",
        "                valid_dataset = HalluDataset(valid_examples, self.tokenizer, self.args.context_len,\n",
        "                                             self.load_model, self.args.task_mode)\n",
        "                valid_dataloader = data.DataLoader(dataset=valid_dataset,\n",
        "                                                   batch_size=self.args.batch_size//2,\n",
        "                                                   shuffle=False,\n",
        "                                                   num_workers=4,\n",
        "                                                   collate_fn=HalluDataset.pad)\n",
        "\n",
        "                self.eval()\n",
        "                predy, validy, hallu_sm_score = [], [], []\n",
        "                valid_loss = 0\n",
        "                for step, batch in enumerate(valid_dataloader):\n",
        "                    batch = tuple(t.to(self.device) for t in batch[:-1])\n",
        "                    input_ids, input_mask, segment_ids, predict_mask, label_ids = batch\n",
        "                    score = fwd_func(input_ids, input_mask, segment_ids, predict_mask)\n",
        "                    hallu_sm = F.softmax(score, dim=1)[:, 1]\n",
        "                    _, pred = torch.max(score, dim=1)\n",
        "                    validy.extend(label_ids.tolist())\n",
        "                    predy.extend(pred.tolist())\n",
        "                    hallu_sm_score.extend(hallu_sm.tolist())\n",
        "                    loss = loss_func(score, label_ids)\n",
        "                    valid_loss += loss.item()\n",
        "                print(\"Valid Epoch {} ...\".format(ei))\n",
        "\n",
        "                acc, f1, precision, recall, gmean, bss, roc_auc, info = \\\n",
        "                    binary_eval(predy, validy, return_f1=True, predscore=hallu_sm_score)\n",
        "\n",
        "                if writer:\n",
        "                    writer.add_scalar('Loss/valid_epoch', valid_loss, ei)\n",
        "                    writer.add_scalar('F1/valid_consistent_epoch', f1[0], ei)\n",
        "                    writer.add_scalar('Precision/valid_consistent_epoch', precision[0], ei)\n",
        "                    writer.add_scalar('Recall/valid_consistent_epoch', recall[0], ei)\n",
        "                    writer.add_scalar('F1/valid_hallucination_epoch', f1[1], ei)\n",
        "                    writer.add_scalar('Precision/valid_hallucination_epoch', precision[1], ei)\n",
        "                    writer.add_scalar('Recall/valid_hallucination_epoch', recall[1], ei)\n",
        "                    writer.add_scalar('Acc/valid_epoch', acc, ei)\n",
        "\n",
        "                if csvwriter:\n",
        "                    rowdata = [ei, precision[1], recall[1], f1[1], precision[0], recall[0], f1[0], gmean, \\\n",
        "                               acc, bss, roc_auc]\n",
        "                    rowdata = [str(f) for f in rowdata]\n",
        "                    csvwriter.writerow(rowdata)\n",
        "\n",
        "                f1_score = f1[0] + f1[1]\n",
        "                if f1_score > best_f1_score:\n",
        "                    best_f1_score = f1_score\n",
        "                    torch.save({\"model_state_dict\": self.state_dict(),\n",
        "                                \"optim_state_dict\": optim.state_dict(),\n",
        "                                \"valid_f1\": f1,\n",
        "                                \"valid_precision\": precision,\n",
        "                                \"valid_recall\": recall,\n",
        "                                \"valid_acc\": acc,\n",
        "                                \"epoch\": epoch},\n",
        "                                prefix + \"best_model.pt\")\n",
        "\n",
        "\n",
        "      def model_train(self, input_ids, input_mask, segment_ids, predict_mask):\n",
        "\n",
        "            if \"xlnet\" in self.load_model:\n",
        "                _, hidden_states = self.model(input_ids=input_ids, attention_mask=input_mask)\n",
        "                hidden_states = [h.transpose(0, 1) for h in hidden_states]\n",
        "            elif \"gpt\" in self.load_model:\n",
        "                _, _, hidden_states = self.model(input_ids=input_ids, attention_mask=input_mask)\n",
        "            else:\n",
        "                model_output = self.model(input_ids=input_ids, attention_mask=input_mask)\n",
        "                prediction_scores = model_output.logits\n",
        "                hidden_states = model_output.hidden_states\n",
        "\n",
        "\n",
        "            features = hidden_states[self.layer]\n",
        "            state = features * predict_mask.unsqueeze(-1)\n",
        "            maxpool_state = 1.0 * torch.max(state, dim=1)[0]\n",
        "            maxpool_state = self.dropout(maxpool_state)\n",
        "            score = self.hidden2label(maxpool_state)\n",
        "\n",
        "            return score\n",
        "\n",
        "\n",
        "      def model_eval(self, model_path, data_path):\n",
        "          dp = DataProcessor()\n",
        "          testpath = data_path\n",
        "          test_examples = dp.get_examples(testpath)\n",
        "          test_dataset = HalluDataset(test_examples, self.tokenizer, self.args.context_len,\n",
        "                                       self.load_model, self.args.task_mode)\n",
        "          test_dataloader = data.DataLoader(dataset=test_dataset,\n",
        "                                             batch_size=self.args.batch_size,\n",
        "                                             shuffle=False,\n",
        "                                             num_workers=4,\n",
        "                                             collate_fn=HalluDataset.pad)\n",
        "\n",
        "          if os.path.exists(model_path):\n",
        "              checkpoint = torch.load(model_path)[\"model_state_dict\"]\n",
        "              model_dict = self.state_dict()\n",
        "              checkpoint = {k: v for k, v in checkpoint.items() if k in model_dict}\n",
        "              model_dict.update(checkpoint)\n",
        "              self.load_state_dict(model_dict)\n",
        "              fwd_func = self.model_train\n",
        "              predy, testy, hallu_sm_score = [], [], []\n",
        "              self.eval()\n",
        "              for step, batch in enumerate(test_dataloader):\n",
        "                  batch = tuple(t.to(self.device) for t in batch[:-1])\n",
        "                  input_ids, input_mask, segment_ids, predict_mask, label_ids = batch\n",
        "                  score = fwd_func(input_ids, input_mask, segment_ids, predict_mask)\n",
        "                  hallu_sm = F.softmax(score, dim=1)[:, 1]\n",
        "                  _, pred = torch.max(score, dim=1)\n",
        "                  testy.extend(label_ids.tolist())\n",
        "                  predy.extend(pred.tolist())\n",
        "                  hallu_sm_score.extend(hallu_sm.tolist())\n",
        "              print(\"Test ...\")\n",
        "\n",
        "              binary_eval(predy, testy, return_f1=True, predscore=hallu_sm_score)\n",
        "          else:\n",
        "              print(\"Invaild model path ...\")\n",
        "\n",
        "args = argparse.Namespace()\n",
        "args.load_model = \"bert-base-multilingual-cased\"\n",
        "args.data_path = \"/content/drive/MyDrive/Wiki-Hades\"\n",
        "args.train_epoch = 10\n",
        "args.batch_size = 8\n",
        "args.lr = 5e-3\n",
        "args.dropout = 0.2\n",
        "args.continue_train = False\n",
        "args.task_mode = \"online\"\n",
        "args.context_len = 200\n",
        "args.bert_layer = -1\n",
        "args.num_epoch = 20\n",
        "args.params = \"\"\n",
        "args.device = \"cuda\"  # Set to \"cpu\" if you want to use CPU instead\n",
        "args.inf_model = \"frozen\"\n",
        "#args.inf_data = \"/content/drive/MyDrive/Wiki-Hades/validDe.jsonl\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YgD3lKq8xRG3"
      },
      "outputs": [],
      "source": [
        "model = ClfModel(args)\n",
        "model.replace_adapter()\n",
        "model = model.to(args.device)\n",
        "\n",
        "learning_rate0 = args.lr\n",
        "weight_decay_finetune = 1e-5\n",
        "\n",
        "if \"all\" in args.params:\n",
        "    named_params = list(model.hidden2label.named_parameters()) + \\\n",
        "                   list(model.model.named_parameters())\n",
        "else:\n",
        "    named_params = list(model.hidden2label.named_parameters())\n",
        "\n",
        "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in named_params if not any(nd in n for nd in no_decay)], 'weight_decay': weight_decay_finetune},\n",
        "    {'params': [p for n, p in named_params if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "]\n",
        "optim_func = torch.optim.Adam if \"gpt\" in args.load_model else BertAdam\n",
        "optimizer = optim_func(optimizer_grouped_parameters, lr=learning_rate0)\n",
        "\n",
        "try:\n",
        "    model.model_run(optimizer)\n",
        "except KeyboardInterrupt:\n",
        "    print(\"Stop by Ctrl-C ...\")\n",
        "\n",
        "model.save_model(\"/content/drive/MyDrive/Modelle/savedModel.pt\")\n",
        "\n",
        "#model.replace_adapter()\n",
        "\n",
        "#model.save_model(\"/content/drive/MyDrive/Modelle/savedModelDe.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gFZ_dZUtfK17"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define a list of model paths\n",
        "model_paths = [\n",
        "    \"/content/drive/MyDrive/Modelle/savedModel.pt\"\n",
        "]\n",
        "\n",
        "results = []\n",
        "\n",
        "for model_path in model_paths:\n",
        "    # Load the models\n",
        "    loaded_model = ClfModel(args)\n",
        "    loaded_model.replace_adapter()\n",
        "    loaded_model.load_state_dict(torch.load(model_path)[\"model_state_dict\"])\n",
        "\n",
        "    # Load the test data\n",
        "    inf_data_path = \"/content/drive/MyDrive/Wiki-Hades/validDe.txt\"  # Ersetzen Sie dies durch den tats√§chlichen Pfad\n",
        "\n",
        "    loaded_model = loaded_model.to(args.device)\n",
        "    result = loaded_model.model_eval(model_path, inf_data_path)\n",
        "\n",
        "    # Add to list\n",
        "    results.append({'Model_Path': model_path, 'Result': result})\n",
        "\n",
        "results_df = pd.DataFrame(results)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
